{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import statements\n",
        "Import LLM and SamplingParams from vLLM. The LLM class is the main class for running offline inference with vLLM engine. The SamplingParams class specifies the parameters for the sampling process.\n"
      ],
      "metadata": {
        "id": "QFJ7AkXVDfPf"
      },
      "id": "QFJ7AkXVDfPf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00cb972-444a-42fe-876c-6b9f118d3b81",
      "metadata": {
        "id": "f00cb972-444a-42fe-876c-6b9f118d3b81",
        "outputId": "da38543f-c7b9-4e4c-b11e-237bf8b22d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-26 08:31:39 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For multi-node inference, please install Ray with `pip install ray`.\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input prompts\n",
        "Define the list of input prompts and the sampling parameters for generation. The sampling temperature is set to 0.8 and the nucleus sampling probability is set to 0.95. For more information about the sampling parameters, refer to the class [definition](https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py)."
      ],
      "metadata": {
        "id": "zLmla7JoD0wj"
      },
      "id": "zLmla7JoD0wj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2700da-e47f-44b1-9a29-76b9cd49850f",
      "metadata": {
        "id": "1b2700da-e47f-44b1-9a29-76b9cd49850f"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize vLLM’s engine\n",
        "Initialize vLLM’s engine for offline inference with the LLM class and the [OPT-125M model](https://arxiv.org/abs/2205.01068). The list of supported models can be found at [supported models](https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models)."
      ],
      "metadata": {
        "id": "SrXNrqWMEPmF"
      },
      "id": "SrXNrqWMEPmF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35ee50c-33b7-4948-ab2c-877af10e7a38",
      "metadata": {
        "id": "d35ee50c-33b7-4948-ab2c-877af10e7a38",
        "outputId": "a3e876ee-3c3d-442c-ca33-28fa29493ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-26 08:31:40 config.py:383] Possibly too large swap space. 4.00 GiB out of the 7.69 GiB total CPU memory is allocated for the swap space.\n",
            "INFO 05-26 08:31:40 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-125m)\n",
            "WARNING 05-26 08:31:40 cpu_executor.py:113] float16 is not supported on CPU, casting to bfloat16.\n",
            "WARNING 05-26 08:31:40 cpu_executor.py:116] CUDA graph is not supported on CPU, fallback to the eager mode.\n",
            "WARNING 05-26 08:31:40 cpu_executor.py:143] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\n",
            "INFO 05-26 08:31:40 selector.py:101] Cannot use _Backend.FLASH_ATTN backend on CPU.\n",
            "INFO 05-26 08:31:40 selector.py:61] Using Torch SDPA backend.\n",
            "INFO 05-26 08:31:40 selector.py:101] Cannot use _Backend.FLASH_ATTN backend on CPU.\n",
            "INFO 05-26 08:31:40 selector.py:61] Using Torch SDPA backend.\n",
            "INFO 05-26 08:31:41 weight_utils.py:207] Using model weights format ['*.bin']\n",
            "INFO 05-26 08:31:42 cpu_executor.py:72] # CPU blocks: 7281\n"
          ]
        }
      ],
      "source": [
        "llm = LLM(model=\"facebook/opt-125m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call llm.generate\n",
        "Call `llm.generate` to generate the outputs. It adds the input prompts to vLLM engine’s waiting queue and executes the vLLM engine to generate the outputs with high throughput. The outputs are returned as a list of `RequestOutput` objects, which include all the output tokens."
      ],
      "metadata": {
        "id": "nB4cLQ9xEk4T"
      },
      "id": "nB4cLQ9xEk4T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b04bc5-8063-43ae-a130-1daf3b0aa051",
      "metadata": {
        "id": "a4b04bc5-8063-43ae-a130-1daf3b0aa051",
        "outputId": "8bc77e17-a538-48fd-e66c-5c7cf3b48a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.51it/s, Generation Speed: 24.23 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Hello, my name is', Generated text: ' Richard and I live in Essex, UK. I am a Licensed Nursing Consultant'\n",
            "Prompt: 'The president of the United States is', Generated text: ' making no secret of his intention to seek a second term.\\n\\nThat move'\n",
            "Prompt: 'The capital of France is', Generated text: ' going to be a fucking wasteland and the less populated areas in the western part of'\n",
            "Prompt: 'The future of AI is', Generated text: \" so dark that its development will seem entirely unwise.   It's currently\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}